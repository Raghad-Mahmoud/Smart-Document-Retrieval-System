{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from elasticsearch import Elasticsearch\n",
    "import spacy\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "from opencage.geocoder import OpenCageGeocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load spaCy model and Connect to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy models for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 1500000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([{\"host\": \"localhost\", \"port\": 9200, \"scheme\": \"http\"}], basic_auth=('elastic', 'CbIxwM6z85Dm6fKtAJte'))\n",
    "index_name = \"reuters\"\n",
    "api_key = '8be579e383db4dc2a7f4895ebf6923d0' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13052\\1587209000.py:94: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  es.indices.create(index=index_name, ignore=400, body=index_settings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'reuters' created successfully.\n"
     ]
    }
   ],
   "source": [
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "    print(f\"Index '{index_name}' deleted.\")\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"autocomplete\": {\n",
    "                    \"tokenizer\": \"autocomplete\",\n",
    "                    \"filter\": [\"lowercase\"]\n",
    "                },\n",
    "                \"custom_content_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"stop\",\n",
    "                        \"custom_stemmer\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"autocomplete\": {\n",
    "                    \"type\": \"edge_ngram\",\n",
    "                    \"min_gram\": 3,\n",
    "                    \"max_gram\": 20,\n",
    "                    \"token_chars\": [\"letter\", \"digit\"]\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"custom_stemmer\": {\n",
    "                    \"type\": \"stemmer\",\n",
    "                    \"name\": \"english\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"autocomplete\",\n",
    "                \"fields\": {\n",
    "                    \"raw\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"custom_content_analyzer\"\n",
    "            },\n",
    "            \"authors\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"first_name\": {\"type\": \"text\"},\n",
    "                    \"last_name\": {\"type\": \"text\"}\n",
    "                }\n",
    "            },\n",
    "            \"date\": {\n",
    "                    \"type\": \"date\",\n",
    "                     \"format\": \"yyyy-MM-dd HH:mm:ss\"\n",
    "                     },\n",
    "            \"georeferences\":{\n",
    "                    \"type\": \"nested\",\n",
    "                    \"properties\": {\n",
    "                    \"expression\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"fields\": {\n",
    "                        \"keyword\": {\n",
    "                            \"type\": \"keyword\"\n",
    "                        }\n",
    "                    }\n",
    "                    }\n",
    "                    }}, \n",
    "            \"geopoint\": {\"type\": \"geo_point\"},  \n",
    "             \n",
    "            \"temporal_expressions\": {\n",
    "                    \"type\": \"nested\",\n",
    "                    \"properties\": {\n",
    "                    \"expression\": {\n",
    "                        \"type\": \"text\"\n",
    "                    }\n",
    "                    }\n",
    "                }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.indices.create(index=index_name, ignore=400, body=index_settings)\n",
    "print(f\"Index '{index_name}' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Temporal expressions and Georeferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temporal_expressions(text):\n",
    "    \"\"\"\n",
    "    Extracts temporal expressions from the given text using spaCy.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains an \"expression\" key representing a temporal expression.\n",
    "    \"\"\"\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract entities labeled as \"DATE\"\n",
    "    temporal_expressions = [{\"expression\": ent.text} for ent in doc.ents if ent.label_ == \"DATE\"]\n",
    "\n",
    "    return temporal_expressions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_georeferences(text):\n",
    "    \"\"\"\n",
    "    Extracts georeferences from the given text using spaCy.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains an \"expression\" key representing a georeference.\n",
    "    \"\"\"\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Use a set to automatically remove duplicate georeferences\n",
    "    georeferences_set = set()\n",
    "\n",
    "    # Extract entities labeled as \"GPE\"\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"GPE\":\n",
    "            georeferences_set.add(ent.text)\n",
    "\n",
    "    # Convert the set to a list of dictionaries\n",
    "    georeferences_list = [{\"expression\": geo} for geo in georeferences_set]\n",
    "\n",
    "    return georeferences_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Text and Format Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    #converting text to lowercase and removing leading/trailing whitespaces\n",
    "    processed_text = text.lower().strip()\n",
    "    return processed_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date(date_string):\n",
    "    parsed_date = datetime.strptime(date_string, '%d-%b-%Y %H:%M:%S.%f')\n",
    "    formatted_date = parsed_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return formatted_date\n",
    "\n",
    "\n",
    "def format_date_to_iso8601(date_string):\n",
    "    try:\n",
    "        # Parse the date string\n",
    "        parsed_date = datetime.strptime(date_string, \"%B %d\")\n",
    "        \n",
    "        # Convert it to ISO 8601 format\n",
    "        iso8601_date = parsed_date.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        \n",
    "        return iso8601_date\n",
    "    except ValueError:\n",
    "        # Handle cases where parsing is not successful\n",
    "        return None\n",
    "\n",
    "def extract_and_clean_month_day(expression):\n",
    "    # Regular expression to match 'Month day' format without unwanted words\n",
    "    month_day_pattern = re.compile(r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}\\b')\n",
    "\n",
    "    # Extract 'Month day' format from the expression\n",
    "    month_day_extraction = re.search(month_day_pattern, expression).group() if re.search(month_day_pattern, expression) else None\n",
    "\n",
    "    # Remove unwanted words from the extraction\n",
    "    cleaned_month_day_extraction = re.sub(r'\\b(?:the\\s+week\\s+ended|the\\s+week|weekly|late this year|the season|last year|recent weeks)\\s+', '', month_day_extraction) if month_day_extraction else None\n",
    "\n",
    "    return cleaned_month_day_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract geopoints and Approximate temporal expressions and Approximate geopoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_geopoints(api_key, address):\n",
    "    \"\"\"\n",
    "    Extracts geopoints (latitude and longitude) from a given address using an external geocoding service.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): API key for the geocoding service.\n",
    "        address (str): The address for which geopoints are to be extracted.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: A dictionary representing the extracted geopoint (latitude and longitude), or None if no geopoint could be extracted.\n",
    "    \"\"\"\n",
    "    # Create an instance of the geocoding service with the provided API key\n",
    "    geocoder = OpenCageGeocode(api_key)\n",
    "\n",
    "    # Make the geocoding request\n",
    "    results = geocoder.geocode(address)\n",
    "\n",
    "    # Check if results are available and not empty\n",
    "    if results and len(results):\n",
    "        # Extract latitude and longitude from the first result\n",
    "        lat = results[0]['geometry']['lat']\n",
    "        lon = results[0]['geometry']['lng']\n",
    "        geopoint = {\"lat\": lat, \"lon\": lon}\n",
    "        return geopoint\n",
    "\n",
    "    # Return None if no geopoint could be extracted\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_temporal_expressions(temporal_expressions):\n",
    "    \"\"\"\n",
    "    Approximates a temporal expression based on a list of temporal expressions.\n",
    "\n",
    "    Args:\n",
    "        temporal_expressions (list): A list of temporal expression dictionaries, where each dictionary contains an \"expression\" key representing a temporal expression.\n",
    "\n",
    "    Returns:\n",
    "        str or None: An ISO 8601 formatted date string representing the first cleaned 'Month day' expression, or None if no valid expression is found.\n",
    "    \"\"\"\n",
    "    # Extract and clean 'Month day' format from each expression\n",
    "    cleaned_extractions = [extract_and_clean_month_day(entry[\"expression\"]) for entry in temporal_expressions]\n",
    "\n",
    "    # Find the first cleaned extraction that is not None\n",
    "    first_cleaned_extraction = next((extraction for extraction in cleaned_extractions if extraction), None)\n",
    "\n",
    "    # If a valid cleaned extraction is found, convert it to ISO 8601 format\n",
    "    if first_cleaned_extraction:\n",
    "        iso8601_date = format_date_to_iso8601(first_cleaned_extraction)\n",
    "        return iso8601_date\n",
    "\n",
    "    # Return None if no valid expression is found\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_geopoint(georeferences, api_key):\n",
    "    \"\"\"\n",
    "    Approximates a geopoint based on a given set of georeferences and an API key.\n",
    "\n",
    "    Args:\n",
    "        georeferences (list): A list of georeference dictionaries, where each dictionary contains an \"expression\" key representing a location.\n",
    "        api_key (str): API key for the geopoint extraction service.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: A dictionary representing the extracted geopoint (latitude and longitude), or None if no valid geopoint could be approximated.\n",
    "    \"\"\"\n",
    "    # Extract the location expression from the georeferences\n",
    "    location = georeferences[0][\"expression\"] if georeferences else None\n",
    "\n",
    "    # If no location is provided, set geopoint to None\n",
    "    if location is None:\n",
    "        geopoint = None\n",
    "    else:\n",
    "        # Attempt to extract geopoint using the provided API key\n",
    "        geopoint = extract_geopoints(api_key, location)\n",
    "\n",
    "    return geopoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Authors and Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_authors(reuters_data):\n",
    "    \"\"\"\n",
    "    Extracts author information from Reuters data.\n",
    "\n",
    "    Args:\n",
    "        reuters_data (str): Input Reuters data containing author information.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains \"First Name\" and \"Last Name\" keys representing author properties.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store author objects\n",
    "    authors = []\n",
    "\n",
    "    # Use a more descriptive regular expression to match author data\n",
    "    author_pattern = re.compile(r'<AUTHOR>(.*?)<\\/AUTHOR>', re.DOTALL)\n",
    "\n",
    "    # Iterate through matches of the author pattern in the Reuters data\n",
    "    for author_match in re.finditer(author_pattern, reuters_data):\n",
    "        author_data = author_match.group(1).strip()\n",
    "\n",
    "        # Use a more robust method to split combined names\n",
    "        names_match = re.search(r'by\\s+([^,]+),\\s*([^<]+)\\s*$', author_data, re.IGNORECASE)\n",
    "        if names_match:\n",
    "            full_name = names_match.group(1).strip()\n",
    "            first_name, _, last_name = full_name.rpartition(' ')\n",
    "        else:\n",
    "            first_name = last_name = ''\n",
    "\n",
    "        # Create a dictionary for each author and add it to the list\n",
    "        author_dict = {'First Name': first_name, 'Last Name': last_name}\n",
    "        authors.append(author_dict)\n",
    "\n",
    "    return authors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_location(dateline_text):\n",
    "    \"\"\"\n",
    "    Extracts location information from dateline text.\n",
    "\n",
    "    Args:\n",
    "        dateline_text (str): Input dateline text.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted location value.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create an XML element from the dateline text\n",
    "        dateline_root = ET.fromstring(f\"<DATELINE>{dateline_text}</DATELINE>\")\n",
    "        \n",
    "        # Extract the location value (first part before the first comma) from the text\n",
    "        location_value = dateline_root.text.split(',')[0].strip() if dateline_root.text is not None else ''\n",
    "        \n",
    "        return location_value\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting location: {e}\")\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexes all documents from an SGML file into Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_single_document(es, index_name, title, content, authors, date, geopoint, temporal_expressions, georeferences):\n",
    "       \n",
    "    document = {\n",
    "        'title': title,\n",
    "        'content': process_text(content),\n",
    "        'authors':authors,\n",
    "        'date': date,\n",
    "        'geopoint': geopoint,\n",
    "        'temporalExpressions': temporal_expressions,\n",
    "        'georeferences': georeferences\n",
    "    }\n",
    "\n",
    "    es.index(index=index_name, body=document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_all_documents(es, index_name, document_path, api_key):\n",
    "    \"\"\"\n",
    "    Indexes all documents from an SGML file into Elasticsearch.\n",
    "\n",
    "    Args:\n",
    "        es: Elasticsearch client instance.\n",
    "        index_name (str): Name of the Elasticsearch index.\n",
    "        document_path (str): Path to the SGML document file.\n",
    "        api_key (str): API key for geopoint extraction service.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(document_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        document_content = ''.join(lines)\n",
    "\n",
    "        # Use regular expressions to find all <REUTERS> elements\n",
    "        reuters_pattern = re.compile(r'<REUTERS.*?>(.*?)<\\/REUTERS>', re.DOTALL)\n",
    "        reuters_matches = re.finditer(reuters_pattern, document_content)\n",
    "\n",
    "        # Get the total number of <REUTERS> elements found\n",
    "        num_reuters_elements = len(list(reuters_matches))\n",
    "        print(f\"Number of <REUTERS> elements found: {num_reuters_elements}\")\n",
    "\n",
    "        # Add a counter to limit the processing to 200 elements\n",
    "        counter = 0\n",
    "\n",
    "        for reuters_match in re.finditer(reuters_pattern, document_content):\n",
    "            # Increment the counter\n",
    "            counter += 1\n",
    "\n",
    "            # Extract relevant information from each <REUTERS> element\n",
    "            reuters_data = reuters_match.group(1).strip()\n",
    "\n",
    "            title_match = re.search(r'<TITLE>(.*?)<\\/TITLE>', reuters_data)\n",
    "            title = title_match.group(1).strip() if title_match else ''\n",
    "\n",
    "            body_match = re.search(r'<BODY>(.*?)<\\/BODY>', reuters_data, re.DOTALL)\n",
    "            content = body_match.group(1).strip() if body_match else ''\n",
    "\n",
    "            authors = extract_authors(reuters_data)\n",
    "\n",
    "            temporal_expressions = extract_temporal_expressions(content)\n",
    "\n",
    "            date_match = re.search(r'<DATE>(.*?)<\\/DATE>', reuters_data)\n",
    "            date = date_match.group(1).strip() if date_match else None\n",
    "            date = format_date(date)\n",
    "\n",
    "            if not date:\n",
    "                date = approximate_temporal_expressions(temporal_expressions)\n",
    "\n",
    "            date_match = re.search(r'<DATELINE>(.*?)<\\/DATELINE>', reuters_data)\n",
    "            date_line = date_match.group(1).strip() if date_match else ''\n",
    "            location_value = extract_location(date_line)\n",
    "            georeferences = extract_georeferences(content)\n",
    "\n",
    "            if not location_value and georeferences:\n",
    "                geopoint = approximate_geopoint(georeferences, api_key)\n",
    "            else:\n",
    "                geopoint = extract_geopoints(api_key, location_value)\n",
    "\n",
    "            # Index the document into Elasticsearch\n",
    "            index_single_document(es, index_name, title, content, authors, date, geopoint, temporal_expressions, georeferences)\n",
    "\n",
    "            # Limit the processing to 200 elements (for testing or other reasons)\n",
    "            if counter == 200:\n",
    "                break\n",
    "\n",
    "        # Refresh the index after indexing all documents\n",
    "        es.indices.refresh(index=index_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing SGML: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of <REUTERS> elements found: 1000\n"
     ]
    }
   ],
   "source": [
    "collection_path = r\"C:\\Users\\hp\\OneDrive\\Desktop\\projectIR\\reut2-009.sgm\" \n",
    "index_all_documents(es, index_name, collection_path,api_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
